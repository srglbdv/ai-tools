# LightRAG for Coolify with PostgreSQL, DozerDB, and MCP Integration

This repository contains a complete Dockerized setup for running LightRAG on Coolify. It uses PostgreSQL for document storage and vector embeddings, DozerDB (a Neo4j variant) for knowledge graph management, and includes a Model Context Protocol (MCP) server for AI assistant integration. The setup is designed to work with existing PostgreSQL or Supabase databases and can integrate with n8n for workflow automation. It uses OpenAI models via their standard APIs.

**Key Features:**
- Complete RAG system with knowledge graph capabilities
- MCP server for direct integration with AI assistants like Claude
- Support for existing PostgreSQL/Supabase databases
- n8n integration for workflow automation
- Coolify deployment with automatic domain generation

## Components

This setup includes:

1. **LightRAG API Server**: Provides Web UI and API support for document indexing, knowledge graph exploration, and RAG queries. Includes built-in support for PostgreSQL storage.
2. **DozerDB (Neo4j variant)**: Stores and manages the knowledge graph with entities and relationships. We specifically chose DozerDB for its ability to create databases, which enhances the flexibility and functionality of the knowledge graph management.
3. **MCP Server**: Provides Model Context Protocol integration for AI assistants like Claude, enabling programmatic interaction with LightRAG through specialized tools.

## Quick Start

### Prerequisites

- Coolify instance
- OpenAI API key
- Existing PostgreSQL or Supabase database

### Setup Instructions

1. In your Coolify instance, create a new Docker Compose service using this repository.

2. Configure the required environment variables:
   - `LLM_BINDING_API_KEY`: Your OpenAI API key
   - `EMBEDDING_BINDING_API_KEY`: Your OpenAI API key (can be the same as above)

   The following variables are automatically generated by Coolify during deployment:
   - `SERVICE_PASSWORD_NEO4J`: Password for the Neo4j database (auto-generated)
   - `SERVICE_PASSWORD_LIGHTRAG`: Password for the LightRAG admin user (auto-generated)
   - `SERVICE_BASE64_LIGHTRAG`: Base64-encoded API key for LightRAG (auto-generated)
   - `SERVICE_REALBASE64_TOKEN`: Base64-encoded secret for JWT tokens (auto-generated)
   - `SERVICE_FQDN_LIGHTRAG_9621`, `SERVICE_FQDN_MCP_3000`: Domain names for services (auto-generated)

3. To use an existing PostgreSQL or Supabase database, configure these variables:
   - `POSTGRES_HOST`: Your PostgreSQL host
   - `POSTGRES_PORT`: Your PostgreSQL port (default: 5432)
   - `POSTGRES_USER`: Your PostgreSQL username
   - `POSTGRES_PASSWORD`: Your PostgreSQL password
   - `POSTGRES_DATABASE`: Your PostgreSQL database name

4. Deploy the service in Coolify.

5. The services will be available at:
   - LightRAG API Server: https://your-coolify-domain.com
   - LightRAG Web UI: https://your-coolify-domain.com/webui

### Initializing LightRAG with Documents

1. Access the LightRAG Web UI at your Coolify domain
2. Log in using the admin credentials you set in the environment variables
3. Upload your documents through the interface
4. Monitor the document indexing process

## Database Configuration

### PostgreSQL/Supabase Integration

This setup supports connecting to existing PostgreSQL or Supabase databases.
- Ensure the Supabase PostgreSQL instance has the pgvector extension enabled
- The LightRAG service will automatically create the necessary tables and indexes

### DozerDB Configuration

This setup uses DozerDB (a Neo4j variant) for knowledge graph storage. DozerDB was specifically chosen for its ability to create databases, which provides enhanced flexibility for knowledge graph management. The configuration includes:

- APOC plugin enabled for advanced graph operations
- File import/export capabilities
- Unrestricted security procedures for full functionality

## MCP Server Integration

This setup includes a Model Context Protocol (MCP) server that provides direct integration with LightRAG through AI assistants like Claude. The MCP server exposes tools that allow AI assistants to interact with your LightRAG deployment programmatically.

### Available MCP Tools

The MCP server provides the following tools:

1. **query_lightrag**: Query the LightRAG knowledge base
2. **list_documents**: List all documents in LightRAG
3. **upload_document**: Upload a document to LightRAG
4. **scan_documents**: Scan for new documents in the document directory
5. **insert_text**: Insert a single text snippet into LightRAG
6. **insert_texts**: Insert multiple text snippets into LightRAG
7. **insert_file**: Insert a single file into LightRAG
8. **insert_file_batch**: Insert multiple files into LightRAG
9. **get_pipeline_status**: Get the status of the document processing pipeline
10. **clear_cache**: Clear the LightRAG query cache
11. **clear_documents**: Clear all documents from LightRAG
12. **query_stream**: Query LightRAG with streaming response
13. **get_graph_labels**: Get all labels in the knowledge graph
14. **get_knowledge_graph**: Get the knowledge graph for a specific label
15. **check_entity_exists**: Check if an entity exists in the knowledge graph
16. **update_entity**: Update an entity in the knowledge graph
17. **update_relation**: Update a relation in the knowledge graph

### Setting Up MCP Integration

The MCP server supports both modern and legacy MCP clients through different endpoints:
- Modern clients: `/mcp` endpoint (recommended)
- Legacy clients: `/sse` endpoint

All endpoints require authentication using a Bearer token. Here's how to connect:

#### Connecting from AI Assistants

To use the MCP server with an AI assistant like Claude:

1. Configure your MCP settings file to connect to the LightRAG MCP server:
   ```json
   {
     "mcpServers": {
       "lightrag": {
         "url": "https://your-coolify-domain.com/mcp",
         "headers": {
           "Authorization": "Bearer your-mcp-api-key"
         },
         "disabled": false,
         "autoApprove": []
       }
     }
   }
   ```

   For legacy clients, use the `/sse` endpoint instead:
   ```json
   {
     "mcpServers": {
       "lightrag": {
         "url": "https://your-coolify-domain.com/sse",
         "headers": {
           "Authorization": "Bearer your-mcp-api-key"
         },
         "disabled": false,
         "autoApprove": []
       }
     }
   }
   ```

2. The AI assistant will now have access to the LightRAG tools and can use them to interact with your knowledge base.


## Integrating with n8n

You can integrate this LightRAG deployment with n8n in two ways: using the MCP integration (recommended) or using direct HTTP requests.

The MCP server is automatically assigned a domain by Coolify through the `SERVICE_FQDN_MCP_3000` environment variable (auto-generated by Coolify), making it accessible at your Coolify domain.

### Method 1: Using MCP Node in n8n (Recommended)

This method uses the built-in MCP integration in n8n for a more user-friendly experience:

1. In your n8n workflow, add an MCP node:
   - Search for "MCP" in the nodes panel
   - Add the MCP node to your workflow

2. Configure a new MCP HTTP connection:
   - **Connection Name**: "LightRAG MCP"
   - **Server URL**: The URL where your MCP server is accessible (e.g., `https://your-coolify-domain.com/sse` for legacy clients or `https://your-coolify-domain.com/mcp` for modern clients)
   - **Authentication**: Select "Bearer Token"
   - **Token**: Your MCP API key (the auto-generated value for SERVICE_BASE64_MCP)

3. Configure the MCP node:
   - Select "LightRAG" as the MCP server
   - Choose the operation you want to perform from the available tools (query_lightrag, list_documents, upload_document, etc.)
   - Configure the parameters for the selected operation
   - Connect the node to other actions in your workflow

This approach provides a more user-friendly interface with proper parameter validation and documentation.

### Method 2: Using HTTP Request Node

For more flexibility or if you prefer direct API access, you can use the HTTP Request node:

1. In your n8n instance, create a new workflow
2. Add an HTTP Request node
3. Configure it to point to the LightRAG API endpoint: `https://your-coolify-domain.com/query`
4. Set the method to POST and body to JSON with the following structure:
   ```json
   {
     "query": "Your question here",
     "mode": "hybrid"
   }
   ```
5. Add the X-API-Key header with your LightRAG API key (the auto-generated value for SERVICE_BASE64_LIGHTRAG)
6. Connect this node to other actions in your workflow

This method gives you more control over the exact API calls but requires more manual configuration.

### API Models

This setup uses the following models by default:
- LLM: `gpt-4.1-mini` from OpenAI
- Embedding: `text-embedding-3-large` from OpenAI

You can modify these in the environment variables to use different models:
- For cheaper usage: Switch to `gpt-3.5-turbo`
- For embedding: Use `text-embedding-3-small` for lower cost
- You can also adjust token limits and other parameters to manage API costs:
  - `MAX_TOKENS`: Maximum tokens for LLM responses (default: 32768)
  - `TEMPERATURE`: Temperature for LLM responses (default: 0.0)
  - `MAX_ASYNC`: Maximum parallel LLM requests (default: 4)

### Document Processing

The setup includes several configuration options for document processing:
- `ENABLE_LLM_CACHE_FOR_EXTRACT`: Enable caching for LLM extraction (default: true)
- `MAX_PARALLEL_INSERT`: Maximum parallel document insertions (default: 2)
- `SUMMARY_LANGUAGE`: Language for document summaries (default: English)
- `CHUNK_SIZE`: Size of document chunks (default: 1200)
- `CHUNK_OVERLAP_SIZE`: Overlap between chunks (default: 100)

## Troubleshooting

### Common Issues

1. **API key issues**: Verify your OpenAI API keys are correctly set in the environment variables
2. **Rate limiting**: If you hit OpenAI rate limits, consider adjusting the MAX_ASYNC parameter
3. **Connection problems**: Check Coolify network configurations and firewall settings
4. **Database connectivity**: If using an external PostgreSQL/Supabase database, ensure it's accessible from the Coolify network
5. **DozerDB issues**: Check the DozerDB logs for any graph database related errors
6. **MCP server issues**: If the MCP server is not responding, check that LightRAG is healthy and that the API key is correctly configured
7. **MCP authentication issues**: Ensure the Bearer token in your MCP connection settings matches exactly with the MCP API key (the auto-generated SERVICE_BASE64_MCP value)

## References

- [LightRAG GitHub Repository](https://github.com/HKUDS/LightRAG)
- [Coolify Documentation](https://coolify.io/docs)
- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)
- [PostgreSQL pgvector Documentation](https://github.com/pgvector/pgvector)
- [DozerDB Documentation](https://dozerdb.org/docs)
- [n8n Documentation](https://docs.n8n.io)
- [Supabase Documentation](https://supabase.com/docs)
- [Model Context Protocol Documentation](https://modelcontextprotocol.github.io/docs/)
