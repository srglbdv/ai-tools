# LightRAG for Coolify with PostgreSQL, DozerDB, and MCP Integration

This repository contains a complete Dockerized setup for running LightRAG on Coolify. It uses PostgreSQL for document storage and vector embeddings, DozerDB (a Neo4j variant) for knowledge graph management, and includes a Model Context Protocol (MCP) server for AI assistant integration. The setup is designed to work with existing PostgreSQL or Supabase databases and can integrate with n8n for workflow automation. It uses OpenAI models via their standard APIs.

**Key Features:**
- Complete RAG system with knowledge graph capabilities
- MCP server for direct integration with AI assistants like Claude
- Support for existing PostgreSQL/Supabase databases
- n8n integration for workflow automation
- Coolify deployment with automatic domain generation

## Components

This setup includes:

1. **LightRAG API Server**: Provides Web UI and API support for document indexing, knowledge graph exploration, and RAG queries. Includes built-in support for PostgreSQL storage.
2. **DozerDB (Neo4j variant)**: Stores and manages the knowledge graph with entities and relationships. We specifically chose DozerDB for its ability to create databases, which enhances the flexibility and functionality of the knowledge graph management.
3. **MCP Server**: Provides Model Context Protocol integration for AI assistants like Claude, enabling programmatic interaction with LightRAG through specialized tools.

## Quick Start

### Prerequisites

- Coolify instance
- OpenAI API key
- Existing PostgreSQL or Supabase database

### Setup Instructions

1. In your Coolify instance, create a new Docker Compose service using this repository.

2. Configure the required environment variables:
   - `LLM_BINDING_API_KEY`: Your OpenAI API key
   - `EMBEDDING_BINDING_API_KEY`: Your OpenAI API key (can be the same as above)

   The following variables are automatically generated by Coolify during deployment:
   - `SERVICE_PASSWORD_NEO4J`: Password for the Neo4j database (auto-generated)
   - `SERVICE_PASSWORD_LIGHTRAG`: Password for the LightRAG admin user (auto-generated)
   - `SERVICE_BASE64_LIGHTRAG`: Base64-encoded API key for LightRAG (auto-generated)
   - `SERVICE_REALBASE64_TOKEN`: Base64-encoded secret for JWT tokens (auto-generated)
   - `SERVICE_FQDN_LIGHTRAG_9621`, `SERVICE_FQDN_MCP_3000`: Domain names for services (auto-generated)

3. To use an existing PostgreSQL or Supabase database, configure these variables:
   - `POSTGRES_HOST`: Your PostgreSQL host
   - `POSTGRES_PORT`: Your PostgreSQL port (default: 5432)
   - `POSTGRES_USER`: Your PostgreSQL username
   - `POSTGRES_PASSWORD`: Your PostgreSQL password
   - `POSTGRES_DATABASE`: Your PostgreSQL database name

4. Deploy the service in Coolify.

5. The services will be available at:
   - LightRAG API Server: https://your-coolify-domain.com
   - LightRAG Web UI: https://your-coolify-domain.com/webui

### Initializing LightRAG with Documents

1. Access the LightRAG Web UI at your Coolify domain
2. Log in using the admin credentials you set in the environment variables
3. Upload your documents through the interface
4. Monitor the document indexing process

## Database Configuration

### PostgreSQL/Supabase Integration

This setup supports connecting to existing PostgreSQL or Supabase databases.
- Ensure the Supabase PostgreSQL instance has the pgvector extension enabled
- The LightRAG service will automatically create the necessary tables and indexes

### DozerDB Configuration

This setup uses DozerDB (a Neo4j variant) for knowledge graph storage. DozerDB was specifically chosen for its ability to create databases, which provides enhanced flexibility for knowledge graph management. The configuration includes:

- APOC plugin enabled for advanced graph operations
- File import/export capabilities
- Unrestricted security procedures for full functionality

## MCP Server Integration

This setup includes a Model Context Protocol (MCP) server that provides direct integration with LightRAG through AI assistants like Claude. The MCP server exposes tools that allow AI assistants to interact with your LightRAG deployment programmatically.

### Available MCP Tools

The MCP server provides the following tools:

1. **query_lightrag**: Query the LightRAG knowledge base
   - Parameters:
     - `query`: The query to send to LightRAG
     - `mode`: Retrieval mode (local, global, hybrid, mix, naive)

2. **list_documents**: List all documents in LightRAG
   - Parameters:
     - `limit`: Maximum number of documents to return (default: 10)
     - `offset`: Number of documents to skip (default: 0)

3. **upload_document**: Upload a document to LightRAG
   - Parameters:
     - `content`: Document content
     - `filename`: Document filename
     - `metadata`: Optional document metadata

4. **get_document_status**: Get the processing status of a document
   - Parameters:
     - `document_id`: Document ID

### Setting Up MCP Integration

The MCP server supports both modern and legacy MCP clients through different endpoints:
- Modern clients: `/mcp` endpoint (recommended)
- Legacy clients: `/sse` endpoint

All endpoints require authentication using a Bearer token. Here's how to connect:

#### Connecting from AI Assistants

To use the MCP server with an AI assistant like Claude:

1. Configure your MCP settings file to connect to the LightRAG MCP server:
   ```json
   {
     "mcpServers": {
       "lightrag": {
         "url": "https://your-coolify-domain.com/mcp",
         "headers": {
           "Authorization": "Bearer your-lightrag-api-key"
         },
         "disabled": false,
         "autoApprove": []
       }
     }
   }
   ```

   For legacy clients, use the `/sse` endpoint instead:
   ```json
   {
     "mcpServers": {
       "lightrag": {
         "url": "https://your-coolify-domain.com/sse",
         "headers": {
           "Authorization": "Bearer your-lightrag-api-key"
         },
         "disabled": false,
         "autoApprove": []
       }
     }
   }
   ```

2. The AI assistant will now have access to the LightRAG tools and can use them to interact with your knowledge base.

#### Connecting from n8n

To connect n8n to the LightRAG MCP server:

1. In your n8n workflow, add an MCP node (already available in n8n by default)
2. Configure a new MCP HTTP connection:
   - **Connection Name**: "LightRAG MCP"
   - **Server URL**: The URL where your MCP server is accessible (e.g., `https://your-coolify-domain.com/sse`)
   - **Authentication**: Select "Bearer Token"
   - **Token**: Your LightRAG API key (the auto-generated value for SERVICE_BASE64_LIGHTRAG)

The MCP server is automatically assigned a domain by Coolify through the `SERVICE_FQDN_MCP_3000` environment variable (auto-generated by Coolify). This makes it accessible at your Coolify domain.

## Integrating with n8n

You can integrate this LightRAG deployment with n8n using the MCP integration:

### Method 1: Using MCP Node in n8n

1. Add an MCP node to your workflow:
   - Search for "MCP" in the nodes panel
   - Add the MCP node to your workflow

2. Configure the MCP node:
   - Select "LightRAG" as the MCP server
   - Choose the operation you want to perform (query, upload document, etc.)
   - Configure the parameters for the selected operation
   - Connect the node to other actions in your workflow

This approach provides a more user-friendly interface for interacting with LightRAG compared to raw HTTP requests.

### Method 2: Using HTTP Request Node

Alternatively, you can still use the HTTP Request node:

1. In your n8n instance, create a new workflow
2. Add an HTTP Request node
3. Configure it to point to the LightRAG API endpoint: `https://your-coolify-domain.com/query`
4. Set the method to POST and body to JSON with the following structure:
   ```json
   {
     "query": "Your question here",
     "mode": "hybrid"
   }
   ```
5. Add the X-API-Key header with your LightRAG API key (the auto-generated value for SERVICE_BASE64_LIGHTRAG)
6. Connect this node to other actions in your workflow

### API Models

This setup uses the following models by default:
- LLM: `gpt-4.1-mini` from OpenAI
- Embedding: `text-embedding-3-large` from OpenAI

You can modify these in the environment variables to use different models:
- For cheaper usage: Switch to `gpt-3.5-turbo`
- For embedding: Use `text-embedding-3-small` for lower cost
- You can also adjust token limits and other parameters to manage API costs:
  - `MAX_TOKENS`: Maximum tokens for LLM responses (default: 32768)
  - `TEMPERATURE`: Temperature for LLM responses (default: 0.0)
  - `MAX_ASYNC`: Maximum parallel LLM requests (default: 4)

### Document Processing

The setup includes several configuration options for document processing:
- `ENABLE_LLM_CACHE_FOR_EXTRACT`: Enable caching for LLM extraction (default: true)
- `MAX_PARALLEL_INSERT`: Maximum parallel document insertions (default: 2)
- `SUMMARY_LANGUAGE`: Language for document summaries (default: English)
- `CHUNK_SIZE`: Size of document chunks (default: 1200)
- `CHUNK_OVERLAP_SIZE`: Overlap between chunks (default: 100)

## Troubleshooting

### Common Issues

1. **API key issues**: Verify your OpenAI API keys are correctly set in the environment variables
2. **Rate limiting**: If you hit OpenAI rate limits, consider adjusting the MAX_ASYNC parameter
3. **Connection problems**: Check Coolify network configurations and firewall settings
4. **Database connectivity**: If using an external PostgreSQL/Supabase database, ensure it's accessible from the Coolify network
5. **DozerDB issues**: Check the DozerDB logs for any graph database related errors
6. **MCP server issues**: If the MCP server is not responding, check that LightRAG is healthy and that the API key is correctly configured
7. **MCP authentication issues**: Ensure the Bearer token in your MCP connection settings matches exactly with the LightRAG API key (the auto-generated SERVICE_BASE64_LIGHTRAG value)

## References

- [LightRAG GitHub Repository](https://github.com/HKUDS/LightRAG)
- [Coolify Documentation](https://coolify.io/docs)
- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)
- [PostgreSQL pgvector Documentation](https://github.com/pgvector/pgvector)
- [DozerDB Documentation](https://dozerdb.org/docs)
- [n8n Documentation](https://docs.n8n.io)
- [Supabase Documentation](https://supabase.com/docs)
- [Model Context Protocol Documentation](https://modelcontextprotocol.github.io/docs/)
