services:
  lightrag:
    build:
      context: .
      dockerfile_inline: |
        # Build stage
        FROM python:3.12-slim-bookworm AS builder

        WORKDIR /app

        # Install Rust and required build dependencies
        RUN apt-get update && apt-get install -y \
            curl \
            build-essential \
            pkg-config \
            git \
            && rm -rf /var/lib/apt/lists/* \
            && curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y \
            && . $HOME/.cargo/env

        # Clone LightRAG repository 
        RUN git clone https://github.com/HKUDS/LightRAG.git .

        # Install dependencies
        ENV PATH="/root/.cargo/bin:${PATH}"
        RUN pip install --user --no-cache-dir -r requirements.txt \
            && pip install --user --no-cache-dir -r lightrag/api/requirements.txt

        # Final stage
        FROM python:3.12-slim-bookworm

        WORKDIR /app

        RUN apt-get update && apt-get install -y \
            curl

        # Copy only necessary files from builder
        COPY --from=builder /root/.local /root/.local
        COPY --from=builder /app/lightrag ./lightrag
        COPY --from=builder /app/setup.py .

        RUN pip install .

        # Make sure scripts in .local are usable
        ENV PATH=/root/.local/bin:$PATH

        # Create necessary directories
        RUN mkdir -p /app/data /app/data/working_dir /app/config

        # Expose the default port
        EXPOSE 9621
    restart: unless-stopped
    depends_on:
      neo4j:
        condition: service_healthy
    volumes:
      - ./data:/app/data
      - ./config:/app/config
    environment:
      - SERVICE_FQDN_LIGHTRAG_9621
      - WEBUI_TITLE=LightRAG
      - WEBUI_DESCRIPTION=Simple and Fast Graph Based RAG System
      # Settings for document indexing
      - ENABLE_LLM_CACHE_FOR_EXTRACT=${ENABLE_LLM_CACHE_FOR_EXTRACT:-true}
      - MAX_PARALLEL_INSERT=${MAX_PARALLEL_INSERT:-2}
      - SUMMARY_LANGUAGE=${SUMMARY_LANGUAGE:-English}
      - CHUNK_SIZE=${CHUNK_SIZE:-1200}
      - CHUNK_OVERLAP_SIZE=${CHUNK_OVERLAP_SIZE:-100}
      # LLM Configuration
      - TIMEOUT=${TIMEOUT:-200}
      - TEMPERATURE=${TEMPERATURE:-0.0}
      - MAX_ASYNC=${MAX_ASYNC:-4}
      - MAX_TOKENS=${MAX_TOKENS:-32768}
      - LLM_BINDING=${LLM_BINDING:-openai}
      - LLM_MODEL=${LLM_MODEL:-gpt-4.1-mini}
      - LLM_BINDING_HOST=${LLM_BINDING_HOST:-https://api.openai.com/v1}
      - LLM_BINDING_API_KEY=${LLM_BINDING_API_KEY}
      # Embedding Configuration - Using OpenAI for embeddings
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-text-embedding-3-large}
      - EMBEDDING_DIM=${EMBEDDING_DIM:-1536}
      - EMBEDDING_BINDING=${EMBEDDING_BINDING:-openai}
      - EMBEDDING_BINDING_HOST=${EMBEDDING_BINDING_HOST:-https://api.openai.com/v1}
      - EMBEDDING_BINDING_API_KEY=${EMBEDDING_BINDING_API_KEY}
      # Storage Configuration
      - LIGHTRAG_KV_STORAGE=PGKVStorage
      - LIGHTRAG_VECTOR_STORAGE=PGVectorStorage
      - LIGHTRAG_GRAPH_STORAGE=Neo4JStorage
      - LIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage
      # PostgreSQL Configuration
      - POSTGRES_HOST=${POSTGRES_HOST:-postgres}
      - POSTGRES_PORT=${POSTGRES_PORT:-5432}
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - POSTGRES_DATABASE=${POSTGRES_DATABASE:-postgres}
      # Neo4j Configuration
      - NEO4J_URI=${NEO4J_URI:-bolt://neo4j:7687}
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=${SERVICE_PASSWORD_NEO4J}
      # Authentication
      - LIGHTRAG_API_KEY=${SERVICE_BASE64_LIGHTRAG}
      - AUTH_ACCOUNTS=admin:${SERVICE_PASSWORD_LIGHTRAG}
      - TOKEN_SECRET=${SERVICE_REALBASE64_TOKEN}
      - TOKEN_EXPIRE_HOURS=${TOKEN_EXPIRE_HOURS:-48}
      - WHITELIST_PATHS=/health,/api/*
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9621/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    entrypoint: ["python", "-m", "lightrag.api.lightrag_server"]

  neo4j:
    build:
      context: .
      dockerfile_inline: |
        FROM graphstack/dozerdb:5.26.3.0
        
        # Install AWS CLI and dependencies for backup functionality
        USER root
        RUN apt-get update && apt-get install -y \
            curl \
            unzip \
            && rm -rf /var/lib/apt/lists/*
        
        # Install AWS CLI v2
        RUN curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip" \
            && unzip awscliv2.zip \
            && ./aws/install \
            && rm -rf awscliv2.zip aws
        
        # Switch back to neo4j user
        USER neo4j
    restart: unless-stopped
    environment:
      - NEO4J_AUTH=neo4j/${SERVICE_PASSWORD_NEO4J}
      - NEO4J_PLUGINS='["apoc"]'
      - NEO4J_apoc_export_file_enabled=true
      - NEO4J_apoc_import_file_enabled=true
      - NEO4J_dbms_security_procedures_unrestricted='*'
      # Backup configuration
      - AWS_S3_BUCKET=${AWS_S3_BUCKET:-}
      - AWS_S3_PREFIX=${AWS_S3_PREFIX:-backups}
      - AWS_S3_REGION=${AWS_S3_REGION:-us-east-1}
      - AWS_S3_ENDPOINT=${AWS_S3_ENDPOINT:-}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-}
      - ADMIN_PASSWORD=${SERVICE_PASSWORD_NEO4J}
    volumes:
      - neo4j-data:/data
      - neo4j-logs:/logs
      - neo4j-plugins:/plugins
      - neo4j-import:/var/lib/neo4j/import
      - type: bind
        source: ./neo4j_backup.sh
        target: /usr/local/bin/neo4j_backup.sh
        content: |
          #!/bin/bash

          # Neo4j Backup Script
          # Dumps Neo4j databases, compresses them, and uploads to S3

          set -euo pipefail

          # Configuration
          AWS_S3_BUCKET="${AWS_S3_BUCKET:-your-backup-bucket}"
          AWS_S3_PREFIX="${AWS_S3_PREFIX:-backups}"
          AWS_S3_REGION="${AWS_S3_REGION:-us-east-1}"
          AWS_S3_ENDPOINT="${AWS_S3_ENDPOINT:-}"  # Optional: for S3-compatible services
          AWS_ACCESS_KEY_ID="${AWS_ACCESS_KEY_ID:-}"
          AWS_SECRET_ACCESS_KEY="${AWS_SECRET_ACCESS_KEY:-}"
          NEO4J_PASSWORD="${ADMIN_PASSWORD:-neo4j}"
          RETENTION_COUNT=3
          LOG_FILE="/logs/backup.log"
          TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
          BACKUP_DIR="/tmp/neo4j-backups"
          BACKUP_NAME="neo4j_backup_${TIMESTAMP}"

          # Logging function
          log() {
              local level=$1
              shift
              local message="$*"
              local log_entry="$(date '+%Y-%m-%d %H:%M:%S') ${level} ${message}"
              echo "${log_entry}" | tee -a "${LOG_FILE}"
          }

          # Error handling with database restart
          cleanup_on_error() {
              local exit_code=$?
              log "ERROR" "Backup process failed with exit code: ${exit_code}"
              
              # Ensure Neo4j service is restarted even on error
              log "INFO" "Attempting to restart Neo4j service after error..."
              restart_database || log "WARN" "Failed to restart Neo4j service after error"
              
              # Clean up temporary files
              if [[ -d "${BACKUP_DIR}/${BACKUP_NAME}" ]]; then
                  rm -rf "${BACKUP_DIR}/${BACKUP_NAME}"
                  log "INFO" "Cleaned up temporary backup directory"
              fi
              
              if [[ -f "${BACKUP_DIR}/${BACKUP_NAME}.tar.gz" ]]; then
                  rm -f "${BACKUP_DIR}/${BACKUP_NAME}.tar.gz"
                  log "INFO" "Cleaned up temporary compressed backup"
              fi
              
              exit ${exit_code}
          }

          trap cleanup_on_error ERR

          # Configure and validate AWS credentials
          configure_aws_credentials() {
              
              # Check if credentials are set
              if [[ -z "${AWS_ACCESS_KEY_ID}" || -z "${AWS_SECRET_ACCESS_KEY}" ]]; then
                  log "ERROR" "AWS credentials not set: AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY required"
                  return 1
              fi
              
              # Configure AWS CLI
              aws configure set aws_access_key_id "${AWS_ACCESS_KEY_ID}"
              aws configure set aws_secret_access_key "${AWS_SECRET_ACCESS_KEY}"
              aws configure set default.region "${AWS_S3_REGION:-us-east-1}"
              
              # Configure custom endpoint if provided
              if [[ -n "${AWS_S3_ENDPOINT}" ]]; then
                  aws configure set default.s3.endpoint_url "${AWS_S3_ENDPOINT}"
                  log "INFO" "Configured custom S3 endpoint: ${AWS_S3_ENDPOINT}"
              fi
              
              log "INFO" "AWS CLI configuration completed"
              
              # Test credentials and bucket access with specific bucket
              if [[ -n "${AWS_S3_ENDPOINT}" ]]; then
                  if aws s3 ls "s3://${AWS_S3_BUCKET}/" --endpoint-url="${AWS_S3_ENDPOINT}" >/dev/null 2>&1; then
                      log "INFO" "AWS credentials and bucket access validated successfully"
                      return 0
                  else
                      log "ERROR" "AWS credentials are invalid, custom S3 endpoint is unreachable, or bucket '${AWS_S3_BUCKET}' is not accessible: ${AWS_S3_ENDPOINT}"
                      return 1
                  fi
              else
                  if aws s3 ls "s3://${AWS_S3_BUCKET}/" >/dev/null 2>&1; then
                      log "INFO" "AWS credentials and bucket access validated successfully"
                      return 0
                  else
                      log "ERROR" "AWS credentials are invalid, AWS S3 service is unreachable, or bucket '${AWS_S3_BUCKET}' is not accessible"
                      return 1
                  fi
              fi
          }

          # Check prerequisites
          check_prerequisites() {
              
              if [[ -z "${NEO4J_PASSWORD}" ]]; then
                  log "ERROR" "NEO4J_PASSWORD is required for cypher-shell authentication"
                  exit 1
              fi
              
              # Configure and validate AWS credentials
              if ! configure_aws_credentials; then
                  log "ERROR" "AWS credential configuration and validation failed"
                  exit 1
              fi
              
              # Create backup directory
              mkdir -p "${BACKUP_DIR}"
              mkdir -p "$(dirname "${LOG_FILE}")"
              
              log "INFO" "Prerequisites check completed successfully"
          }

          # Stop a specific database
          stop_database() {
              local db_name=$1

              if cypher-shell -u neo4j -p "${NEO4J_PASSWORD}" "STOP DATABASE \`${db_name}\`" 2>&1 | tee -a "${LOG_FILE}"; then
                  # Wait a moment for database to fully start
                  sleep 10

                  log "INFO" "Successfully stopped ${db_name} database"
              else
                  log "ERROR" "Failed to stop ${db_name} database"
                  return 1
              fi
          }

          # Start a specific database using cypher-shell
          start_database() {
              local db_name=$1
              
              if cypher-shell -u neo4j -p "${NEO4J_PASSWORD}" "START DATABASE \`${db_name}\`" 2>&1 | tee -a "${LOG_FILE}"; then
                  # Wait a moment for database to fully start
                  sleep 10

                  log "INFO" "Successfully started ${db_name} database"
              else
                  log "ERROR" "Failed to start ${db_name} database"
                  return 1
              fi
          }

          # Restart Neo4j service using neo4j restart command
          restart_database() {
              log "INFO" "Restarting Neo4j service"
              
              if neo4j restart 2>&1 | tee -a "${LOG_FILE}"; then
                  # Wait longer for service restart
                  sleep 30

                  log "INFO" "Successfully restarted Neo4j service"
              else
                  log "ERROR" "Failed to restart Neo4j service"
                  return 1
              fi
          }

          # Dump a specific database
          dump_database() {
              local db_name=$1
              log "INFO" "Dumping ${db_name} database to: ${BACKUP_DIR}/${BACKUP_NAME}"
              
              # Create backup directory if it doesn't exist
              mkdir -p "${BACKUP_DIR}/${BACKUP_NAME}"
              
              if neo4j-admin database dump "${db_name}" --to-path="${BACKUP_DIR}/${BACKUP_NAME}" 2>&1 | tee -a "${LOG_FILE}"; then
                  log "INFO" "Successfully dumped ${db_name} database"
              else
                  log "ERROR" "Failed to dump ${db_name} database"
                  return 1
              fi
          }

          # Backup a single database
          backup_database() {
              local db_name=$1
              log "INFO" "Starting backup for database: ${db_name}"
              
              # Start the database (ensure it's running)
              if ! start_database "${db_name}"; then
                  log "ERROR" "Failed to start ${db_name} database before backup"
                  return 1
              fi
              
              # Stop the database
              if ! stop_database "${db_name}"; then
                  log "ERROR" "Failed to stop ${db_name} database"
                  return 1
              fi
              
              # Dump the database
              if ! dump_database "${db_name}"; then
                  log "ERROR" "Failed to dump ${db_name} database"
                  return 1
              fi
              
              
              log "INFO" "Completed backup for database: ${db_name}"
          }

          # Compress backup
          compress_backup() {
              log "INFO" "Compressing backup to: ${BACKUP_DIR}/${BACKUP_NAME}.tar.gz"
              
              if tar -czf "${BACKUP_DIR}/${BACKUP_NAME}.tar.gz" -C "${BACKUP_DIR}/${BACKUP_NAME}" . 2>&1 | tee -a "${LOG_FILE}"; then
                  log "INFO" "Successfully compressed backup"
                  
                  # Get compressed file size for logging
                  local file_size
                  file_size=$(du -h "${BACKUP_DIR}/${BACKUP_NAME}.tar.gz" | cut -f1)
                  log "INFO" "Compressed backup size: ${file_size}"
                  
                  # Remove uncompressed backup directory
                  rm -rf "${BACKUP_DIR}/${BACKUP_NAME}"
                  log "INFO" "Cleaned up uncompressed backup directory"
              else
                  log "ERROR" "Failed to compress backup"
                  return 1
              fi
          }

          # Upload to S3
          upload_to_s3() {
              local s3_key="${AWS_S3_PREFIX}/${BACKUP_NAME}.tar.gz"
              
              log "INFO" "Uploading compressed backup to S3: s3://${AWS_S3_BUCKET}/${s3_key}"
              
              # Build upload command with endpoint if needed
              # Add CRC32 checksum for compatibility with Cloudflare R2 and other S3-compatible services
              if [[ -n "${AWS_S3_ENDPOINT}" ]]; then
                  if aws s3 cp "${BACKUP_DIR}/${BACKUP_NAME}.tar.gz" "s3://${AWS_S3_BUCKET}/${s3_key}" --endpoint-url="${AWS_S3_ENDPOINT}" --checksum-algorithm CRC32 2>&1 | tee -a "${LOG_FILE}"; then
                      log "INFO" "Successfully uploaded backup to S3"
                      
                      # Remove local compressed backup after successful upload
                      rm -f "${BACKUP_DIR}/${BACKUP_NAME}.tar.gz"
                      log "INFO" "Cleaned up local compressed backup"
                  else
                      log "ERROR" "Failed to upload backup to S3"
                      return 1
                  fi
              else
                  if aws s3 cp "${BACKUP_DIR}/${BACKUP_NAME}.tar.gz" "s3://${AWS_S3_BUCKET}/${s3_key}" 2>&1 | tee -a "${LOG_FILE}"; then
                      log "INFO" "Successfully uploaded backup to S3"
                      
                      # Remove local compressed backup after successful upload
                      rm -f "${BACKUP_DIR}/${BACKUP_NAME}.tar.gz"
                      log "INFO" "Cleaned up local compressed backup"
                  else
                      log "ERROR" "Failed to upload backup to S3"
                      return 1
                  fi
              fi
          }

          # Clean up old backups in S3
          cleanup_old_backups() {
              log "INFO" "Cleaning up old backups (keeping last ${RETENTION_COUNT} versions)..."
              
              # Prepare endpoint arguments
              local endpoint_args=""
              if [[ -n "${AWS_S3_ENDPOINT}" ]]; then
                  endpoint_args="--endpoint-url=${AWS_S3_ENDPOINT}"
              fi
              
              # List all backup files in S3, sorted by date (newest first)
              local backup_objects
              backup_objects=$(aws s3api list-objects-v2 \
                  --bucket "${AWS_S3_BUCKET}" \
                  --prefix "${AWS_S3_PREFIX}/neo4j_backup_" \
                  ${endpoint_args} \
                  --query 'Contents[?ends_with(Key, `.tar.gz`)].Key' \
                  --output text 2>/dev/null | tr '\t' '\n' | sort -r || true)
              
              if [[ -z "${backup_objects}" ]]; then
                  log "INFO" "No existing backups found in S3"
                  return 0
              fi
              
              local count=0
              local objects_to_delete=()
              
              while IFS= read -r object; do
                  [[ -z "${object}" ]] && continue
                  count=$((count + 1))
                  if [[ ${count} -gt ${RETENTION_COUNT} ]]; then
                      objects_to_delete+=("${object}")
                  fi
              done <<< "${backup_objects}"
              
              if [[ ${#objects_to_delete[@]} -gt 0 ]]; then
                  log "INFO" "Deleting ${#objects_to_delete[@]} old backup(s)..."
                  for object in "${objects_to_delete[@]}"; do
                      if [[ -n "${AWS_S3_ENDPOINT}" ]]; then
                          if aws s3 rm "s3://${AWS_S3_BUCKET}/${object}" --endpoint-url="${AWS_S3_ENDPOINT}" 2>&1 | tee -a "${LOG_FILE}"; then
                              log "INFO" "Deleted old backup: ${object}"
                          else
                              log "WARN" "Failed to delete old backup: ${object}"
                          fi
                      else
                          if aws s3 rm "s3://${AWS_S3_BUCKET}/${object}" 2>&1 | tee -a "${LOG_FILE}"; then
                              log "INFO" "Deleted old backup: ${object}"
                          else
                              log "WARN" "Failed to delete old backup: ${object}"
                          fi
                      fi
                  done
              else
                  log "INFO" "No old backups to clean up"
              fi
          }

          # Main execution
          main() {
              log "INFO" "Starting Neo4j backup process"
              log "INFO" "Backup name: ${BACKUP_NAME}"
              log "INFO" "Target bucket: s3://${AWS_S3_BUCKET}/${AWS_S3_PREFIX}"
              
              # Check prerequisites
              check_prerequisites
              
              # Backup each database individually
              if ! backup_database "chunk-entity-relation"; then
                  log "ERROR" "Failed to backup chunk-entity-relation database"
                  exit 1
              fi
              
              if ! backup_database "neo4j"; then
                  log "ERROR" "Failed to backup neo4j database"
                  exit 1
              fi
              
              # Compress backup
              if ! compress_backup; then
                  log "ERROR" "Backup compression failed"
                  exit 1
              fi
              
              # Upload to S3
              if ! upload_to_s3; then
                  log "ERROR" "S3 upload failed"
                  exit 1
              fi
              
              # Clean up old backups
              cleanup_old_backups
              
              # Restart Neo4j service after all backups are complete
              if ! restart_database; then
                  log "ERROR" "Failed to restart Neo4j service after backup completion"
                  exit 1
              fi
              
          }

          # Execute main function
          main "$@"
    healthcheck:
      test: wget http://localhost:7474 || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  mcp:
    build:
      context: .
      dockerfile_inline: |
        # Build stage
        FROM node:22-slim AS builder
        
        WORKDIR /app
        
        # Install dependencies in a single layer to reduce image size
        RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*
        
        # Install dependencies including express for HTTP server
        COPY ./mcp/package.json .

        # Install dependencies
        RUN npm install
        
        # Create TypeScript config and source files
        COPY ./mcp/tsconfig.json .
        COPY ./mcp/src ./src
        
        # Build TypeScript code
        RUN npm run build
        
        # Runtime stage
        FROM node:22-slim
        
        WORKDIR /app
        
        # Install only production dependencies
        COPY ./mcp/package.json .
        RUN npm install --production && \
            apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*
        
        # Copy built files from builder stage
        COPY --from=builder /app/build ./build
        
        # Expose HTTP port
        EXPOSE 3000
    volumes:
      - type: bind
        source: ./mcp/package.json
        target: /app/package.json
        content: |
          {
            "name": "lightrag-mcp-server",
            "version": "0.1.0",
            "type": "module",
            "scripts": {
              "build": "tsc && chmod +x build/index.js",
              "start": "node build/index.js"
            },
            "dependencies": {
              "@modelcontextprotocol/sdk": "^1.11.0",
              "axios": "^1.6.0",
              "express": "^4.18.2",
              "form-data": "^4.0.0",
              "zod": "^3.22.4"
            },
            "devDependencies": {
              "@types/express": "^4.17.17",
              "@types/node": "^20.4.5",
              "typescript": "^5.0.0"
            }
          }
      - type: bind
        source: ./mcp/tsconfig.json
        target: /app/tsconfig.json
        content: |
          {
            "compilerOptions": {
              "target": "ES2020",
              "module": "NodeNext",
              "moduleResolution": "NodeNext",
              "esModuleInterop": true,
              "outDir": "build",
              "strict": true,
              "skipLibCheck": true,
              "resolveJsonModule": true
            },
            "include": ["src/**/*"]
          }
      - type: bind
        source: ./mcp/src/index.ts
        target: /app/src/index.ts
        content: |
          #!/usr/bin/env node
          import { McpServer } from "@modelcontextprotocol/sdk/server/mcp.js";
          import { StreamableHTTPServerTransport } from "@modelcontextprotocol/sdk/server/streamableHttp.js";
          import { SSEServerTransport } from "@modelcontextprotocol/sdk/server/sse.js";
          import { Request, Response } from "express";
          import express from "express";
          import axios from "axios";
          import FormData from "form-data";
          import { z } from "zod";

          const API_URL = process.env.LIGHTRAG_API_URL || "http://lightrag:9621";
          const LIGHTRAG_API_KEY = process.env.LIGHTRAG_API_KEY;
          const MCP_API_KEY = process.env.MCP_API_KEY;
          const PORT = process.env.PORT || 3000;

          if (!LIGHTRAG_API_KEY) {
            throw new Error("LIGHTRAG_API_KEY environment variable is required");
          }

          if (!MCP_API_KEY) {
            throw new Error("MCP_API_KEY environment variable is required");
          }

          // Create a single reusable function to get a configured axios instance
          function getLightRAGClient(isFormData = false) {
            const headers: Record<string, string> = {
              "X-API-Key": LIGHTRAG_API_KEY as string,
            };
            
            // Add Content-Type header only for non-form data requests
            if (!isFormData) {
              headers["Content-Type"] = "application/json";
            }
            
            return axios.create({
              baseURL: API_URL,
              headers,
            });
          }

          // Helper function to format response data into MCP content format
          function formatResponse(data: any, isError: boolean = false) {
            return {
              content: [
                {
                  type: "text",
                  text: JSON.stringify(data, null, 2)
                }
              ],
              isError
            };
          }

          // Store transports for each session type
          const transports = {
            streamable: {} as Record<string, StreamableHTTPServerTransport>,
            sse: {} as Record<string, SSEServerTransport>
          };

          // Function to create a new server instance
          function getServer() {
            const server = new McpServer({
              name: "lightrag-mcp-server",
              version: "0.1.0",
            });
            
            // Set up tool handlers
            server.tool(
              "query_lightrag",
              {
                query: z.string().describe("The query to send to LightRAG"),
                conversation_history: z.array(z.record(z.any()))
                  .describe("Conversation history")
                  .optional(),
                ids: z.array(z.string())
                  .describe("Specific document IDs to query")
                  .optional()
              },
              async (args) => {
                try {
                  const client = getLightRAGClient();
                  
                  // Read environment variables for query parameters
                  const queryParams = {
                    query: args.query,
                    conversation_history: args.conversation_history,
                    ids: args.ids,
                    mode: process.env.MCP_QUERY_MODE || "hybrid",
                    only_need_context: process.env.MCP_QUERY_ONLY_NEED_CONTEXT === "true",
                    only_need_prompt: process.env.MCP_QUERY_ONLY_NEED_PROMPT === "true",
                    response_type: process.env.MCP_QUERY_RESPONSE_TYPE,
                    top_k: process.env.MCP_QUERY_TOP_K ? parseInt(process.env.MCP_QUERY_TOP_K) : undefined,
                    max_token_for_text_unit: process.env.MCP_QUERY_MAX_TOKEN_FOR_TEXT_UNIT ? 
                      parseInt(process.env.MCP_QUERY_MAX_TOKEN_FOR_TEXT_UNIT) : undefined,
                    max_token_for_global_context: process.env.MCP_QUERY_MAX_TOKEN_FOR_GLOBAL_CONTEXT ? 
                      parseInt(process.env.MCP_QUERY_MAX_TOKEN_FOR_GLOBAL_CONTEXT) : undefined,
                    max_token_for_local_context: process.env.MCP_QUERY_MAX_TOKEN_FOR_LOCAL_CONTEXT ? 
                      parseInt(process.env.MCP_QUERY_MAX_TOKEN_FOR_LOCAL_CONTEXT) : undefined,
                    history_turns: process.env.MCP_QUERY_HISTORY_TURNS ? 
                      parseInt(process.env.MCP_QUERY_HISTORY_TURNS) : undefined,
                    user_prompt: process.env.MCP_QUERY_USER_PROMPT
                  };
                  
                  // Filter out undefined values and empty arrays
                  const filteredParams = Object.fromEntries(
                    Object.entries(queryParams).filter(([k, v]) => {
                      if (v === undefined) return false;
                      if (Array.isArray(v) && v.length === 0) return false;
                      return true;
                    })
                  );

                  const response = await client.post("/query", filteredParams);
                  
                  return {
                    content: [
                      {
                        type: "text",
                        text: JSON.stringify(response.data, null, 2)
                      }
                    ]
                  };
                } catch (error: any) {
                  return {
                    content: [
                      {
                        type: "text",
                        text: `Error querying LightRAG: ${error.message}`
                      }
                    ],
                    isError: true
                  };
                }
              }
            );
            
            server.tool(
              "list_documents",
              {},
              async (args) => {
                try {
                  const client = getLightRAGClient();
                  
                  const response = await client.get("/documents");
                  
                  return {
                    content: [
                      {
                        type: "text",
                        text: JSON.stringify(response.data, null, 2)
                      }
                    ]
                  };
                } catch (error: any) {
                  return {
                    content: [
                      {
                        type: "text",
                        text: `Error listing documents: ${error.message}`
                      }
                    ],
                    isError: true
                  };
                }
              }
            );
            
            server.tool(
              "upload_document",
              {
                content: z.string().describe("Document content"),
                filename: z.string().describe("Document filename")
              },
              async (args) => {
                try {
                  const { content, filename } = args;
                  const client = getLightRAGClient(true);
                  
                  const formData = new FormData();
                  
                  // Create a Buffer from the content string
                  const buffer = Buffer.from(content);
                  formData.append("file", buffer, filename);
                  
                  const response = await client.post("/documents/upload", formData, {
                    headers: {
                      ...formData.getHeaders(),
                    },
                  });
                  
                  return {
                    content: [
                      {
                        type: "text",
                        text: JSON.stringify(response.data, null, 2)
                      }
                    ]
                  };
                } catch (error: any) {
                  return {
                    content: [
                      {
                        type: "text",
                        text: `Error uploading document: ${error.message}`
                      }
                    ],
                    isError: true
                  };
                }
              }
            );

            // Add scan_documents tool
            server.tool(
              "scan_documents",
              {},
              async (args) => {
                try {
                  const client = getLightRAGClient();
                  
                  const response = await client.post("/documents/scan");
                  
                  return {
                    content: [
                      {
                        type: "text",
                        text: JSON.stringify(response.data, null, 2)
                      }
                    ]
                  };
                } catch (error: any) {
                  return {
                    content: [
                      {
                        type: "text",
                        text: `Error scanning documents: ${error.message}`
                      }
                    ],
                    isError: true
                  };
                }
              }
            );
            
            // Add insert_text tool
            server.tool(
              "insert_text",
              {
                text: z.string().describe("The text to insert")
              },
              async (args) => {
                try {
                  const { text } = args;
                  const client = getLightRAGClient();
                  
                  const response = await client.post("/documents/text", {
                    text
                  });
                  
                  return {
                    content: [
                      {
                        type: "text",
                        text: JSON.stringify(response.data, null, 2)
                      }
                    ]
                  };
                } catch (error: any) {
                  return {
                    content: [
                      {
                        type: "text",
                        text: `Error inserting text: ${error.message}`
                      }
                    ],
                    isError: true
                  };
                }
              }
            );
            
            // Add insert_texts tool
            server.tool(
              "insert_texts",
              {
                texts: z.array(z.string()).describe("List of texts to insert")
              },
              async (args) => {
                try {
                  const { texts } = args;
                  const client = getLightRAGClient();
                  
                  const response = await client.post("/documents/texts", {
                    texts
                  });
                  
                  return {
                    content: [
                      {
                        type: "text",
                        text: JSON.stringify(response.data, null, 2)
                      }
                    ]
                  };
                } catch (error: any) {
                  return {
                    content: [
                      {
                        type: "text",
                        text: `Error inserting texts: ${error.message}`
                      }
                    ],
                    isError: true
                  };
                }
              }
            );
            
            // Add insert_file tool
            server.tool(
              "insert_file",
              {
                content: z.string().describe("File content"),
                filename: z.string().describe("File name")
              },
              async (args) => {
                try {
                  const { content, filename } = args;
                  const client = getLightRAGClient(true);
                  
                  const formData = new FormData();
                  
                  // Create a Buffer from the content string
                  const buffer = Buffer.from(content);
                  formData.append("file", buffer, filename);
                  
                  const response = await client.post("/documents/file", formData, {
                    headers: {
                      ...formData.getHeaders(),
                    },
                  });
                  
                  return {
                    content: [
                      {
                        type: "text",
                        text: JSON.stringify(response.data, null, 2)
                      }
                    ]
                  };
                } catch (error: any) {
                  return {
                    content: [
                      {
                        type: "text",
                        text: `Error inserting file: ${error.message}`
                      }
                    ],
                    isError: true
                  };
                }
              }
            );
            
            // Add insert_file_batch tool
            server.tool(
              "insert_file_batch",
              {
                files: z.array(
                  z.object({
                    content: z.string().describe("File content"),
                    filename: z.string().describe("File name")
                  })
                ).describe("List of files to process")
              },
              async (args) => {
                try {
                  const { files } = args;
                  const client = getLightRAGClient(true);
                  
                  const formData = new FormData();
                  
                  // Add each file to the form data
                  files.forEach((file, index) => {
                    const buffer = Buffer.from(file.content);
                    formData.append("files", buffer, file.filename);
                  });
                  
                  const response = await client.post("/documents/file_batch", formData, {
                    headers: {
                      ...formData.getHeaders(),
                    },
                  });
                  
                  return {
                    content: [
                      {
                        type: "text",
                        text: JSON.stringify(response.data, null, 2)
                      }
                    ]
                  };
                } catch (error: any) {
                  return {
                    content: [
                      {
                        type: "text",
                        text: `Error inserting file batch: ${error.message}`
                      }
                    ],
                    isError: true
                  };
                }
              }
            );
            
            // Add get_pipeline_status tool
            server.tool(
              "get_pipeline_status",
              {},
              async (args) => {
                try {
                  const client = getLightRAGClient();
                  
                  const response = await client.get("/documents/pipeline_status");
                  
                  return {
                    content: [
                      {
                        type: "text",
                        text: JSON.stringify(response.data, null, 2)
                      }
                    ]
                  };
                } catch (error: any) {
                  return {
                    content: [
                      {
                        type: "text",
                        text: `Error getting pipeline status: ${error.message}`
                      }
                    ],
                    isError: true
                  };
                }
              }
            );
            
            // Add clear_cache tool
            server.tool(
              "clear_cache",
              {
                modes: z.array(
                  z.enum(["default", "naive", "local", "global", "hybrid", "mix"])
                ).describe("Modes of cache to clear").optional()
              },
              async (args) => {
                try {
                  const { modes } = args;
                  const client = getLightRAGClient();
                  
                  const response = await client.post("/documents/clear_cache", {
                    modes
                  });
                  
                  return {
                    content: [
                      {
                        type: "text",
                        text: JSON.stringify(response.data, null, 2)
                      }
                    ]
                  };
                } catch (error: any) {
                  return {
                    content: [
                      {
                        type: "text",
                        text: `Error clearing cache: ${error.message}`
                      }
                    ],
                    isError: true
                  };
                }
              }
            );
            
            // Add clear_documents tool
            server.tool(
              "clear_documents",
              {},
              async (args) => {
                try {
                  const client = getLightRAGClient();
                  
                  const response = await client.delete("/documents");
                  
                  return {
                    content: [
                      {
                        type: "text",
                        text: JSON.stringify(response.data, null, 2)
                      }
                    ]
                  };
                } catch (error: any) {
                  return {
                    content: [
                      {
                        type: "text",
                        text: `Error clearing documents: ${error.message}`
                      }
                    ],
                    isError: true
                  };
                }
              }
            );
            
            // Add query_stream tool
            server.tool(
              "query_stream",
              {
                query: z.string().describe("The query to send to LightRAG"),
                conversation_history: z.array(z.record(z.any()))
                  .describe("Conversation history")
                  .optional(),
                ids: z.array(z.string())
                  .describe("Specific document IDs to query")
                  .optional()
              },
              async (args) => {
                try {
                  const client = getLightRAGClient();
                  
                  // Read environment variables for query parameters
                  const queryParams = {
                    query: args.query,
                    conversation_history: args.conversation_history,
                    ids: args.ids,
                    mode: process.env.MCP_QUERY_MODE || "hybrid",
                    only_need_context: process.env.MCP_QUERY_ONLY_NEED_CONTEXT === "true",
                    only_need_prompt: process.env.MCP_QUERY_ONLY_NEED_PROMPT === "true",
                    response_type: process.env.MCP_QUERY_RESPONSE_TYPE,
                    top_k: process.env.MCP_QUERY_TOP_K ? parseInt(process.env.MCP_QUERY_TOP_K) : undefined,
                    max_token_for_text_unit: process.env.MCP_QUERY_MAX_TOKEN_FOR_TEXT_UNIT ? 
                      parseInt(process.env.MCP_QUERY_MAX_TOKEN_FOR_TEXT_UNIT) : undefined,
                    max_token_for_global_context: process.env.MCP_QUERY_MAX_TOKEN_FOR_GLOBAL_CONTEXT ? 
                      parseInt(process.env.MCP_QUERY_MAX_TOKEN_FOR_GLOBAL_CONTEXT) : undefined,
                    max_token_for_local_context: process.env.MCP_QUERY_MAX_TOKEN_FOR_LOCAL_CONTEXT ? 
                      parseInt(process.env.MCP_QUERY_MAX_TOKEN_FOR_LOCAL_CONTEXT) : undefined,
                    history_turns: process.env.MCP_QUERY_HISTORY_TURNS ? 
                      parseInt(process.env.MCP_QUERY_HISTORY_TURNS) : undefined,
                    user_prompt: process.env.MCP_QUERY_USER_PROMPT
                  };
                  
                  // Filter out undefined values and empty arrays
                  const filteredParams = Object.fromEntries(
                    Object.entries(queryParams).filter(([k, v]) => {
                      if (v === undefined) return false;
                      if (Array.isArray(v) && v.length === 0) return false;
                      return true;
                    })
                  );
                  
                  // Note: This is a simplified implementation as streaming requires special handling
                  const response = await client.post("/query/stream", filteredParams);
                  
                  return {
                    content: [
                      {
                        type: "text",
                        text: JSON.stringify(response.data, null, 2)
                      }
                    ]
                  };
                } catch (error: any) {
                  return {
                    content: [
                      {
                        type: "text",
                        text: `Error streaming query: ${error.message}`
                      }
                    ],
                    isError: true
                  };
                }
              }
            );
            
            // Add get_graph_labels tool
            server.tool(
              "get_graph_labels",
              {},
              async (args) => {
                try {
                  const client = getLightRAGClient();
                  
                  const response = await client.get("/graph/label/list");
                  
                  return {
                    content: [
                      {
                        type: "text",
                        text: JSON.stringify(response.data, null, 2)
                      }
                    ]
                  };
                } catch (error: any) {
                  return {
                    content: [
                      {
                        type: "text",
                        text: `Error getting graph labels: ${error.message}`
                      }
                    ],
                    isError: true
                  };
                }
              }
            );
            
            // Add get_knowledge_graph tool
            server.tool(
              "get_knowledge_graph",
              {
                label: z.string().describe("Label to get knowledge graph for"),
                max_depth: z.number().min(1).describe("Maximum depth of graph").default(3).optional(),
                max_nodes: z.number().min(1).describe("Maximum nodes to return").default(1000).optional()
              },
              async (args) => {
                try {
                  const { label, max_depth, max_nodes } = args;
                  const client = getLightRAGClient();
                  
                  const response = await client.get("/graphs", {
                    params: {
                      label,
                      max_depth: max_depth || 3,
                      max_nodes: max_nodes || 1000
                    }
                  });
                  
                  return {
                    content: [
                      {
                        type: "text",
                        text: JSON.stringify(response.data, null, 2)
                      }
                    ]
                  };
                } catch (error: any) {
                  return {
                    content: [
                      {
                        type: "text",
                        text: `Error getting knowledge graph: ${error.message}`
                      }
                    ],
                    isError: true
                  };
                }
              }
            );
            
            // Add check_entity_exists tool
            server.tool(
              "check_entity_exists",
              {
                name: z.string().describe("Entity name to check")
              },
              async (args) => {
                try {
                  const { name } = args;
                  const client = getLightRAGClient();
                  
                  const response = await client.get("/graph/entity/exists", {
                    params: { name }
                  });
                  
                  return {
                    content: [
                      {
                        type: "text",
                        text: JSON.stringify(response.data, null, 2)
                      }
                    ]
                  };
                } catch (error: any) {
                  return {
                    content: [
                      {
                        type: "text",
                        text: `Error checking entity existence: ${error.message}`
                      }
                    ],
                    isError: true
                  };
                }
              }
            );
            
            // Add update_entity tool
            server.tool(
              "update_entity",
              {
                entity_name: z.string().describe("Entity name"),
                updated_data: z.record(z.any()).describe("Updated data"),
                allow_rename: z.boolean().describe("Allow rename").default(false).optional()
              },
              async (args) => {
                try {
                  const { entity_name, updated_data, allow_rename } = args;
                  const client = getLightRAGClient();
                  
                  const response = await client.post("/graph/entity/edit", {
                    entity_name,
                    updated_data,
                    allow_rename: allow_rename || false
                  });
                  
                  return {
                    content: [
                      {
                        type: "text",
                        text: JSON.stringify(response.data, null, 2)
                      }
                    ]
                  };
                } catch (error: any) {
                  return {
                    content: [
                      {
                        type: "text",
                        text: `Error updating entity: ${error.message}`
                      }
                    ],
                    isError: true
                  };
                }
              }
            );
            
            // Add update_relation tool
            server.tool(
              "update_relation",
              {
                source_id: z.string().describe("Source ID"),
                target_id: z.string().describe("Target ID"),
                updated_data: z.record(z.any()).describe("Updated data")
              },
              async (args) => {
                try {
                  const { source_id, target_id, updated_data } = args;
                  const client = getLightRAGClient();
                  
                  const response = await client.post("/graph/relation/edit", {
                    source_id,
                    target_id,
                    updated_data
                  });
                  
                  return {
                    content: [
                      {
                        type: "text",
                        text: JSON.stringify(response.data, null, 2)
                      }
                    ]
                  };
                } catch (error: any) {
                  return {
                    content: [
                      {
                        type: "text",
                        text: `Error updating relation: ${error.message}`
                      }
                    ],
                    isError: true
                  };
                }
              }
            );
            
            return server;
          }

          // Create Express app
          const app = express();
          app.use(express.json());

          // Add health check endpoint
          app.get('/health', (req: Request, res: Response) => {
            res.status(200).send('OK');
          });

          // Add authentication middleware
          function authenticate(req: Request, res: Response, next: Function) {
            const authHeader = req.headers['authorization'];
            if (!authHeader || authHeader !== `Bearer ${MCP_API_KEY}`) {
              return res.status(401).json({
                jsonrpc: '2.0',
                error: {
                  code: -32001,
                  message: 'Unauthorized',
                },
                id: null,
              });
            }
            next();
          }

          // Simple logging function
          function log(message: string, level: 'info' | 'warn' | 'error' = 'info') {
            const timestamp = new Date().toISOString();
            const prefix = level === 'error' ? 'ERROR' : level === 'warn' ? 'WARNING' : 'INFO';
            console.log(`${prefix}: ${message}`);
          }

          // MCP endpoint with authentication - POST for client-to-server communication
          app.post('/mcp', authenticate, async (req: Request, res: Response) => {
            // In stateless mode, create a new instance of transport and server for each request
            // to ensure complete isolation. A single instance would cause request ID collisions
            // when multiple clients connect concurrently.
            
            const requestId = Math.random().toString(36).substring(2, 15);
            log(`Received POST request to /mcp [ID: ${requestId}]`);
            
            try {
              const server = getServer(); 
              log(`Server instance created successfully [ID: ${requestId}]`);
              
              const transport = new StreamableHTTPServerTransport({
                sessionIdGenerator: undefined,
              });
              log(`Transport created successfully [ID: ${requestId}]`);
              
              res.on('close', () => {
                log(`Request closed [ID: ${requestId}]`);
                transport.close();
                server.close();
              });
              
              await server.connect(transport);
              log(`Server connected to transport [ID: ${requestId}]`);
              
              log(`Handling request [ID: ${requestId}]`);
              await transport.handleRequest(req, res, req.body);
              log(`Request handled successfully [ID: ${requestId}]`);
            } catch (error) {
              log(`Error handling MCP request [ID: ${requestId}]: ${error}`, 'error');
              if (!res.headersSent) {
                res.status(500).json({
                  jsonrpc: '2.0',
                  error: {
                    code: -32603,
                    message: 'Internal server error',
                  },
                  id: null,
                });
              }
            }
          });

          // MCP endpoint with authentication - GET for server-to-client notifications and tool discovery
          app.get('/mcp', authenticate, async (req: Request, res: Response) => {
            const requestId = Math.random().toString(36).substring(2, 15);
            log(`Received GET request to /mcp [ID: ${requestId}]`);
            
            try {
              // Create a server instance
              const server = getServer();
              log(`Server instance created successfully for tool discovery [ID: ${requestId}]`);
              
              // Respond with a simple success message for modern clients
              // This is a simplified response since we don't have access to getMetadata, getTools, etc.
              res.status(200).json({
                jsonrpc: "2.0",
                result: {
                  server: {
                    name: "lightrag-mcp-server",
                    version: "0.1.0"
                  }
                },
                id: null
              });
              
              log(`Tool discovery response sent successfully [ID: ${requestId}]`);
              
              // Clean up
              server.close();
              
            } catch (error) {
              log(`Error handling GET request [ID: ${requestId}]: ${error}`, 'error');
              if (!res.headersSent) {
                res.status(500).json({
                  jsonrpc: '2.0',
                  error: {
                    code: -32603,
                    message: 'Internal server error',
                  },
                  id: null,
                });
              }
            }
          });

          // MCP endpoint with authentication - DELETE for session termination
          app.delete('/mcp', authenticate, async (req: Request, res: Response) => {
            const requestId = Math.random().toString(36).substring(2, 15);
            log(`Received DELETE request to /mcp [ID: ${requestId}]`);
            log(`DELETE method not allowed in stateless mode [ID: ${requestId}]`, 'warn');
            
            res.writeHead(405).end(JSON.stringify({
              jsonrpc: "2.0",
              error: {
                code: -32000,
                message: "Method not allowed in stateless mode."
              },
              id: null
            }));
          });

          // Legacy SSE endpoint for older clients
          app.get('/sse', authenticate, async (req: Request, res: Response) => {
            const requestId = Math.random().toString(36).substring(2, 15);
            log(`Received GET request to /sse [ID: ${requestId}]`);
            
            try {
              const server = getServer();
              log(`Server instance created successfully for SSE [ID: ${requestId}]`);
              
              // Create SSE transport for legacy clients
              const transport = new SSEServerTransport('/messages', res);
              transports.sse[transport.sessionId] = transport;
              
              res.on("close", () => {
                log(`SSE connection closed [ID: ${requestId}]`);
                delete transports.sse[transport.sessionId];
                server.close();
              });
              
              await server.connect(transport);
              log(`Server connected to SSE transport [ID: ${requestId}]`);
            } catch (error) {
              log(`Error handling SSE request [ID: ${requestId}]: ${error}`, 'error');
              if (!res.headersSent) {
                res.status(500).send('Internal server error');
              }
            }
          });

          // Legacy message endpoint for older clients
          app.post('/messages', authenticate, async (req: Request, res: Response) => {
            const sessionId = req.query.sessionId as string;
            const requestId = Math.random().toString(36).substring(2, 15);
            log(`Received POST request to /messages with sessionId ${sessionId} [ID: ${requestId}]`);
            
            const transport = transports.sse[sessionId];
            if (transport) {
              try {
                await transport.handlePostMessage(req, res, req.body);
                log(`Message handled successfully [ID: ${requestId}]`);
              } catch (error) {
                log(`Error handling message [ID: ${requestId}]: ${error}`, 'error');
                if (!res.headersSent) {
                  res.status(500).json({
                    jsonrpc: '2.0',
                    error: {
                      code: -32603,
                      message: 'Internal server error',
                    },
                    id: null,
                  });
                }
              }
            } else {
              log(`No transport found for sessionId ${sessionId} [ID: ${requestId}]`, 'warn');
              res.status(400).json({
                jsonrpc: '2.0',
                error: {
                  code: -32000,
                  message: 'No transport found for sessionId',
                },
                id: null,
              });
            }
          });

          // Start the server
          app.listen(PORT, () => {
            log(`LightRAG MCP Server started successfully`);
            log(`Server is listening on port ${PORT}`);
            log(`Health check endpoint available at http://localhost:${PORT}/health`);
            log(`MCP endpoint available at http://localhost:${PORT}/mcp`);
            log(`Legacy SSE endpoint available at http://localhost:${PORT}/sse`);
            log(`Legacy messages endpoint available at http://localhost:${PORT}/messages`);
            log(`Server is ready to accept requests`);
          });

    environment:
      - LIGHTRAG_API_URL=http://lightrag:9621
      - LIGHTRAG_API_KEY=${SERVICE_BASE64_LIGHTRAG}
      - MCP_API_KEY=${SERVICE_BASE64_MCP}
      - PORT=3000
      - SERVICE_FQDN_MCP_3000
      # Query parameters with default values
      - MCP_QUERY_MODE=${MCP_QUERY_MODE:-hybrid}
      - MCP_QUERY_ONLY_NEED_CONTEXT=${MCP_QUERY_ONLY_NEED_CONTEXT:-false}
      - MCP_QUERY_ONLY_NEED_PROMPT=${MCP_QUERY_ONLY_NEED_PROMPT:-false}
      - MCP_QUERY_RESPONSE_TYPE=${MCP_QUERY_RESPONSE_TYPE:-Multiple Paragraphs}
      - MCP_QUERY_TOP_K=${MCP_QUERY_TOP_K:-10}
      - MCP_QUERY_MAX_TOKEN_FOR_TEXT_UNIT=${MCP_QUERY_MAX_TOKEN_FOR_TEXT_UNIT:-4000}
      - MCP_QUERY_MAX_TOKEN_FOR_GLOBAL_CONTEXT=${MCP_QUERY_MAX_TOKEN_FOR_GLOBAL_CONTEXT:-4000}
      - MCP_QUERY_MAX_TOKEN_FOR_LOCAL_CONTEXT=${MCP_QUERY_MAX_TOKEN_FOR_LOCAL_CONTEXT:-4000}
      - MCP_QUERY_HISTORY_TURNS=${MCP_QUERY_HISTORY_TURNS:-3}
      - MCP_QUERY_USER_PROMPT=${MCP_QUERY_USER_PROMPT:-}
    entrypoint: ["npm", "start"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
volumes:
  neo4j-data: null
  neo4j-logs: null
  neo4j-plugins: null
  neo4j-import: null
